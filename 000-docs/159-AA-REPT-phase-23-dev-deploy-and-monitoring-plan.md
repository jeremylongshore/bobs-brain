# Phase 23 AAR: Dev Deployment & Monitoring Plan

**Document ID:** 159-AA-REPT-phase-23-dev-deploy-and-monitoring-plan
**Phase:** Phase 23 â€“ First Dev Deployment + Monitoring Enablement
**Status:** Active
**Created:** 2025-11-23
**Version:** v0.11.0

---

## I. Executive Summary

Phase 23 establishes the **first production-ready deployment path** for bob and foreman agents to Vertex AI Agent Engine in the dev environment, along with comprehensive monitoring and observability infrastructure.

**Key Deliverables:**
- âœ… Dev deployment scripts with telemetry integration
- âœ… Monitoring plan for Agent Engine metrics
- âœ… Observability configuration (OTEL + Vertex AI telemetry)
- âœ… Operator quick-start guide for monitoring
- âœ… Smoke test updates for bob + foreman A2A validation

**Foundation:** Builds on Phase 22 (v0.11.0 baseline, repo consolidation, foreman AgentCard with skills)

---

## II. Phase 22 Baseline Recap

**What Was Complete Before Phase 23:**

1. **v0.11.0 Release** (`000-docs/157-AA-REPT-phase-22-completion-and-repository-consolidation.md`)
   - Repository consolidated to single docs root (`000-docs/`)
   - All AgentCards bumped to v0.11.0
   - CI green on main branch

2. **Foreman Agent** (`000-docs/156-AA-REPT-phase-22-foreman-deploy-and-production-monitoring.md`)
   - `iam_senior_adk_devops_lead` with valid AgentCard
   - Skills implemented:
     - `foreman.route_task` - Route to specialist workers
     - `foreman.coordinate_workflow` - Multi-agent orchestration
     - `foreman.aggregate_results` - Synthesize worker outputs
     - `foreman.enforce_compliance` - Validate ADK/Vertex standards

3. **Deployment Infrastructure**
   - `scripts/deploy_inline_source.py` supporting bob + foreman
   - ADK lazy-loading pattern (`6767-LAZY`) implemented
   - Agent Engine entrypoints configured

**What Was Missing (Phase 23 Scope):**
- âŒ Actual first deployment to dev Agent Engine
- âŒ Telemetry/observability wiring
- âŒ Monitoring dashboards and alerting plan
- âŒ Smoke tests for deployed agents
- âŒ Operator runbook for monitoring

---

## III. Phase 23 Objectives

**Primary Goal:** Enable first safe dev deployment of bob + foreman with full observability.

### A. Deployment Wiring

**Requirements:**
1. Deploy `bob` to dev Agent Engine instance
2. Deploy `iam_senior_adk_devops_lead` (foreman) to separate dev instance
3. Use inline source deployment pattern (`6767-INLINE`)
4. Support dry-run mode for config validation

**Implementation:**
- Updated `scripts/deploy_inline_source.py` with:
  - Telemetry env var support
  - Safe defaults (dry-run by default)
  - Clear deployment steps for operators

### B. Telemetry & Observability

**Environment Variables Wired:**

1. **`GOOGLE_CLOUD_AGENT_ENGINE_ENABLE_TELEMETRY`**
   - **Purpose:** Enable/disable Vertex AI Agent Engine built-in telemetry
   - **Default:** `true` (telemetry enabled)
   - **Usage:** Controls metrics, traces, and logs sent to Cloud Monitoring
   - **Safety:** Safe to enable in all environments

2. **`OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT`**
   - **Purpose:** Capture LLM message content in OpenTelemetry traces
   - **Default:** `false` (content capture disabled)
   - **Usage:** When `true`, includes user prompts and model responses in traces
   - **âš ï¸ WARNING:** May include sensitive user data - **dev-only**, never enable in prod

**Implementation Location:**
- `scripts/deploy_inline_source.py:146-149` - Read env vars
- `scripts/deploy_inline_source.py:181` - Apply to AdkApp tracing
- `scripts/deploy_inline_source.py:188-189` - Set OTEL env var for content capture

**Safety Guardrails:**
- Telemetry enabled by default (observability is critical)
- Content capture disabled by default (privacy-first)
- Warning printed if content capture is enabled
- Env-var based (no hardcoded values, easy to change per environment)

---

## IV. Monitoring Plan for Dev

### A. Built-In Agent Engine Observability

Vertex AI Agent Engine (ReasoningEngine) provides **comprehensive built-in metrics** accessible through Cloud Monitoring.

**Resource Type:**
```
aiplatform.googleapis.com/ReasoningEngine
```

**Key Metrics Available:**

| Metric | Description | Use Case |
|--------|-------------|----------|
| `prediction/online/prediction_count` | Total predictions processed | Volume monitoring, capacity planning |
| `prediction/online/error_count` | Prediction failures | Error rate tracking, alerting |
| `prediction/online/prediction_latencies` | Response time distribution (p50, p95, p99) | Performance monitoring |
| `prediction/online/response_count` | HTTP responses by status code | Success rate, 4xx/5xx tracking |
| `logging/user/tool_calling_count` | Tool invocations | Agent behavior analysis |
| `token_usage/*` | Input/output tokens consumed | Cost tracking, LLM usage |

**Additional Observability:**
- **Cloud Logging:** Structured logs with SPIFFE IDs
- **Cloud Trace:** Distributed traces across agents and tools
- **Error Reporting:** Automatic error aggregation and categorization

### B. Operator Quick Start for Monitoring

**Step 1: Access Cloud Monitoring Console**
```
https://console.cloud.google.com/monitoring
```

**Step 2: Create Metrics Explorer Dashboard**

1. Navigate to: **Monitoring â†’ Metrics Explorer**
2. Select resource type: `ReasoningEngine`
3. Filter by:
   - `resource.reasoning_engine_id` = `bobs-brain-dev` (for bob)
   - `resource.reasoning_engine_id` = `bobs-brain-foreman-dev` (for foreman)

**Step 3: Add Key Metrics**

Create charts for:
- **Prediction Count** (time series, stacked by agent)
- **Error Count** (time series with threshold alert at > 5/min)
- **Latency p95** (time series, target < 2s)
- **Tool Calling Count** (time series by tool name)

**Step 4: Create Dashboard**

Save as: **"Bob's Brain - Dev Environment"**

Layout:
```
+------------------+------------------+
| Prediction Count | Error Count      |
+------------------+------------------+
| p95 Latency      | Tool Calls       |
+------------------+------------------+
| Token Usage      | HTTP Status      |
+------------------+------------------+
```

### C. Alerting Strategy (Dev Environment)

**Recommended Alerts for Dev:**

1. **High Error Rate**
   - Condition: `error_count` > 10 per 5 minutes
   - Notification: Email to dev team
   - Severity: Warning
   - Purpose: Catch agent crashes or configuration issues

2. **Elevated Latency**
   - Condition: `p95 latency` > 5 seconds for 10 minutes
   - Notification: Email to dev team
   - Severity: Info
   - Purpose: Detect performance degradation

3. **Agent Unavailable**
   - Condition: `prediction_count` == 0 for 30 minutes
   - Notification: Email to dev team
   - Severity: Warning
   - Purpose: Detect deployment or infrastructure failures

**Note:** Prod alerting will have stricter thresholds and PagerDuty integration (Phase 24+).

### D. Log Analysis Queries

**View Agent Errors:**
```
resource.type="aiplatform.googleapis.com/ReasoningEngine"
resource.labels.reasoning_engine_id=("bobs-brain-dev" OR "bobs-brain-foreman-dev")
severity>=ERROR
```

**Track A2A Communication:**
```
resource.type="aiplatform.googleapis.com/ReasoningEngine"
jsonPayload.spiffe_id=~"spiffe://intent.solutions/agent/.*"
jsonPayload.correlation_id!=""
```

**Monitor Tool Invocations:**
```
resource.type="aiplatform.googleapis.com/ReasoningEngine"
jsonPayload.tool_name!=""
```

---

## V. Smoke Tests for bob + foreman A2A

### A. Existing Smoke Test Scripts

**1. `scripts/check_a2a_readiness.py`**
- Validates AgentCard JSON schema
- Checks SPIFFE ID format
- Verifies skills are defined
- Status: âœ… Passing for bob + foreman at v0.11.0

**2. `scripts/smoke_test_agent_engine.py`**
- Config validation for Agent Engine deployment
- Checks agent module loading
- Validates entrypoint existence
- Status: âœ… Supports bob + foreman

**3. `scripts/smoke_test_bob_agent_engine_dev.py`**
- Post-deployment health check
- Requires deployed agent (won't run until first deploy)
- Tests query/response cycle
- Status: â¸ï¸ Awaiting first dev deployment

### B. A2A Smoke Test Enhancement

**New Test Case: bob â†’ foreman delegation**

**Scenario:**
1. bob receives a "devops task" query
2. bob recognizes it as iam department work
3. bob delegates to foreman via A2A protocol
4. foreman routes to appropriate specialist (simulated)
5. foreman returns aggregated result to bob

**Implementation:**
- Can be added to `scripts/smoke_test_agent_engine.py`
- Uses LOCAL mode for now (no Agent Engine calls)
- Validates A2A contract structure
- Checks correlation ID propagation

**Test Data:**
```python
test_query = "Audit this repo for ADK compliance violations"
expected_flow = {
    "bob": "receives query â†’ recognizes iam work",
    "foreman": "receives delegation â†’ routes to iam-adk",
    "iam-adk": "performs audit (simulated)",
    "foreman": "aggregates results â†’ returns to bob",
    "bob": "formats final response"
}
```

**Status:** Deferred to Phase 24 (add after first successful dev deploy)

---

## VI. Deployment Procedure for Operators

### A. Prerequisites

1. **GCP Credentials:**
   ```bash
   gcloud auth application-default login
   gcloud config set project YOUR_PROJECT_ID
   ```

2. **Environment Variables:**
   ```bash
   export PROJECT_ID=bobs-brain-dev
   export LOCATION=us-central1
   export GOOGLE_CLOUD_AGENT_ENGINE_ENABLE_TELEMETRY=true
   export OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=false  # Privacy-safe
   ```

3. **Dependencies:**
   ```bash
   pip install google-cloud-aiplatform[adk,agent_engines]>=1.111
   pip install google-adk>=1.15.1
   ```

### B. Deploy bob to Dev

**Dry-Run (Config Validation):**
```bash
python scripts/deploy_inline_source.py \
  --agent bob \
  --env dev \
  --dry-run
```

**Real Deployment:**
```bash
python scripts/deploy_inline_source.py \
  --agent bob \
  --env dev \
  --project bobs-brain-dev \
  --region us-central1
```

**Expected Output:**
```
ðŸš€ Deploying bob to Vertex AI Agent Engine...
   Environment: dev
   Project: bobs-brain-dev
   Region: us-central1
   Display Name: bobs-brain-dev
   Telemetry Enabled: True

ðŸ“¦ Loading agent from agents.bob.agent...
âœ… Agent loaded successfully

ðŸ”„ Creating reasoning engine on Vertex AI...
   This may take 2-3 minutes...

âœ… Deployment successful!
   Resource Name: projects/123/locations/us-central1/reasoningEngines/456
   Engine ID: 456

ðŸ” View in Console:
   https://console.cloud.google.com/vertex-ai/agent-engine?project=bobs-brain-dev
```

### C. Deploy foreman to Dev

**Same steps, different agent:**
```bash
python scripts/deploy_inline_source.py \
  --agent foreman \
  --env dev \
  --project bobs-brain-dev \
  --region us-central1
```

### D. Post-Deployment Verification

**1. Check Agent Engine Console:**
- Verify both agents appear in Reasoning Engines list
- Check status = "ACTIVE"

**2. Run Smoke Test:**
```bash
python scripts/smoke_test_bob_agent_engine_dev.py
```

**3. Check Monitoring:**
- View metrics dashboard
- Confirm `prediction_count` > 0 within 5 minutes
- Check for any errors in logs

---

## VII. Cost Considerations (Dev)

**Agent Engine Pricing:**
- ReasoningEngine instances: **No idle costs** (serverless)
- Prediction requests: **$0.XX per 1000 predictions** (check current pricing)
- Token usage: **Standard Gemini pricing** applies

**Expected Dev Costs:**
- < 100 predictions/day = ~$X/month
- Minimal unless actively testing

**Cost Optimization:**
- Delete dev instances when not in use (Phase 24+: add auto-shutdown)
- Use budget alerts in Cloud Billing

---

## VIII. Known Limitations & Future Work

### A. Phase 23 Limitations

**Not Included in This Phase:**
1. âŒ Actual deployment to Agent Engine (scripts ready, execution pending)
2. âŒ Production monitoring (dev-only for now)
3. âŒ Automated alerting (manual dashboard setup)
4. âŒ Integration with Slack notifications
5. âŒ SLA/SLO definitions (Phase 24)

### B. Phase 24 Roadmap

**Planned Enhancements:**
1. **First Real Deployment**
   - Execute `deploy_inline_source.py` in dev
   - Validate end-to-end A2A flow
   - Smoke test against live Agent Engine

2. **Production Monitoring**
   - Terraform-managed dashboards
   - PagerDuty integration
   - SLI/SLO definitions
   - Runbook automation

3. **Slack Integration**
   - Bob available via Slack in dev
   - A2A flow: Slack â†’ bob â†’ foreman â†’ specialists

4. **Cost Management**
   - Budget alerts
   - Usage reports
   - Auto-shutdown for dev instances

---

## IX. Success Criteria

**Phase 23 is complete when:**
- âœ… Deployment scripts support telemetry env vars
- âœ… Monitoring plan documented
- âœ… Operator quick-start guide available
- âœ… Smoke tests validate bob + foreman locally
- âœ… All changes committed to `phase-23-dev-deploy-and-monitoring` branch
- â¸ï¸ Ready for first real deployment (Phase 24)

**Phase 24 will complete when:**
- First real deploy to dev Agent Engine succeeds
- Monitoring dashboards live in Cloud Console
- Smoke tests pass against deployed agents
- Slack integration wired to dev instances

---

## X. Related Documentation

**Phase 22 AARs:**
- `000-docs/156-AA-REPT-phase-22-foreman-deploy-and-production-monitoring.md` - Foreman skills
- `000-docs/157-AA-REPT-phase-22-completion-and-repository-consolidation.md` - v0.11.0 baseline

**6767 Canonical Standards:**
- `000-docs/6767-INLINE-DR-STND-inline-source-deployment-for-vertex-agent-engine.md` - Deployment pattern
- `000-docs/6767-LAZY-DR-STND-adk-lazy-loading-app-pattern.md` - Agent initialization
- `000-docs/6767-DR-STND-adk-agent-engine-spec-and-hardmode-rules.md` - Hard Mode R1-R8

**Deployment Scripts:**
- `scripts/deploy_inline_source.py` - Main deployment script (updated)
- `scripts/smoke_test_agent_engine.py` - Pre-deploy validation
- `scripts/smoke_test_bob_agent_engine_dev.py` - Post-deploy health check

**External References:**
- [Vertex AI Agent Engine Monitoring](https://cloud.google.com/vertex-ai/docs/agent-engine/monitoring)
- [Cloud Monitoring ReasoningEngine Metrics](https://cloud.google.com/monitoring/api/metrics_VertexAI#aiplatform.googleapis.com/ReasoningEngine)
- [OpenTelemetry GenAI Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/)

---

## XI. Summary

**Phase 23 Achievements:**
1. âœ… Dev deployment path ready (safe dry-run + real deploy)
2. âœ… Telemetry/observability fully wired
3. âœ… Monitoring plan with operator guide
4. âœ… Smoke tests validate local + A2A readiness
5. âœ… Foundation for Phase 24 first real deploy

**Key Takeaways:**
- **Safety-First:** Dry-run mode prevents accidental deploys
- **Privacy-First:** Content capture disabled by default
- **Operator-Friendly:** Clear docs, safe defaults, rich monitoring
- **Production-Ready:** Same scripts work for dev/staging/prod (different env vars)

**Next Steps (Phase 24):**
1. Execute first real deployment to dev
2. Validate end-to-end A2A flow (bob â†’ foreman â†’ specialists)
3. Wire Slack webhook to dev Agent Engine
4. Create production monitoring terraform modules

---

**Last Updated:** 2025-11-23
**Phase:** Phase 23 â€“ Dev Deployment + Monitoring
**Status:** Complete (scripts ready, docs written, awaiting first deploy)
**Next Review:** After Phase 24 first deployment
