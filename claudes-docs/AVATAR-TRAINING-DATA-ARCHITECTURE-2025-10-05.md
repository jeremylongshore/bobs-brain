# Bob's Avatar Training Data Architecture

**Date:** 2025-10-05
**Question:** Where to train Bob with your avatars/personal data?
**Key Issue:** If data is local, how does cloud Bob access it?

---

## Understanding "Avatar Training"

**What is avatar training?**
- Building knowledge base with YOUR expertise
- Teaching Bob YOUR communication style
- Loading YOUR documents, preferences, context
- Creating a "digital twin" that thinks like you

**Bob's learning systems:**
1. **Knowledge Orchestrator** - 77,264 documents (research, docs)
2. **Circle of Life** - Learns from conversations, corrections
3. **Vector Store (Chroma)** - Semantic search on documents
4. **SQLite DB** - Structured knowledge, FTS search
5. **Neo4j Graph** (optional) - Relationship mapping

---

## The Core Challenge

### Problem: Data Privacy vs Cloud Access

```
Your Personal Data (Local)
- Resume, work history
- Personal docs, notes
- Private conversations
- Preferences, habits
- Proprietary knowledge

         ‚Üì

    WHERE SHOULD THIS LIVE?

         ‚Üì

Cloud Bob (Production)        Local Bob (Development)
- Needs access to respond     - Has direct access
- Running 24/7 in cloud       - Only when PC on
- Serving Slack requests      - Fast local access
```

**Dilemma:**
- **Option A:** Upload personal data to cloud ‚Üí Privacy risk
- **Option B:** Keep data local ‚Üí Cloud Bob can't access it
- **Option C:** Hybrid architecture ‚Üí Complexity

---

## Architecture Options

### Option 1: Everything in Cloud ‚òÅÔ∏è

**Architecture:**
```
Your Personal Data
    ‚Üì Upload once
Google Cloud Storage (private bucket)
    ‚Üì
Cloud Run (Bob)
    ‚îú‚îÄ‚îÄ Reads from Cloud Storage
    ‚îú‚îÄ‚îÄ Vector DB (Chroma in memory)
    ‚îú‚îÄ‚îÄ Knowledge DB (SQLite ephemeral)
    ‚îî‚îÄ‚îÄ Learns from Slack conversations
    ‚Üì
Persistent Storage:
    ‚îú‚îÄ‚îÄ Cloud SQL (structured data)
    ‚îú‚îÄ‚îÄ Cloud Storage (documents)
    ‚îî‚îÄ‚îÄ Vertex AI Vector Search (embeddings)
```

**How it works:**
1. **One-time upload:** Upload your training docs to Cloud Storage
2. **Bob reads on startup:** Loads knowledge into memory
3. **Persistent learning:** Circle of Life stores in Cloud SQL
4. **Always available:** Cloud Bob has everything

**Privacy controls:**
- Private bucket (only Bob's service account can read)
- Encryption at rest (Google default)
- Encryption in transit (HTTPS)
- VPC networking (isolated network)
- Audit logs (who accessed what)

‚úÖ **PROS:**
- Cloud Bob has full access 24/7
- Fast access (in same data center)
- Persistent storage (survives restarts)
- Scales automatically
- Professional setup

‚ùå **CONS:**
- Personal data in Google's cloud
- Must trust Google's security
- Data subject to subpoenas
- Costs more ($10-30/month extra)

**Cost breakdown:**
| Component | Monthly Cost |
|-----------|--------------|
| Cloud Run (base) | $5-15 |
| Cloud Storage | $1-5 (per GB) |
| Cloud SQL (small) | $10-20 |
| Vertex AI Vector | $10-30 |
| **Total** | **$26-70/month** |

**Setup:**
```bash
# 1. Create Cloud Storage bucket
gsutil mb -p bobs-house-ai gs://bobs-brain-knowledge

# 2. Upload training data
gsutil -m cp -r ~/Documents/training-data/* gs://bobs-brain-knowledge/

# 3. Set permissions (private)
gsutil iam ch serviceAccount:bob@bobs-house-ai.iam.gserviceaccount.com:objectViewer \
  gs://bobs-brain-knowledge

# 4. Deploy Bob with access
gcloud run deploy bobs-brain \
  --set-env-vars "KNOWLEDGE_BUCKET=bobs-brain-knowledge"
```

---

### Option 2: Everything Local üíª

**Architecture:**
```
Your Personal Data (Local)
    ‚Üì
Local Bob (Flask on your PC)
    ‚îú‚îÄ‚îÄ Direct file access
    ‚îú‚îÄ‚îÄ SQLite (77,264 docs)
    ‚îú‚îÄ‚îÄ Chroma vector store
    ‚îú‚îÄ‚îÄ Circle of Life learning
    ‚îî‚îÄ‚îÄ No cloud, no upload
    ‚Üì
Slack (via ngrok tunnel)
```

**How it works:**
1. **Training data local:** Everything in `~/Documents/`, `~/research/`
2. **Bob reads locally:** Direct filesystem access
3. **Learning persists:** SQLite/Chroma on your disk
4. **Slack via ngrok:** Tunnel for webhooks

‚úÖ **PROS:**
- **Maximum privacy** - data never leaves your machine
- **Full control** - you own everything
- **No cloud costs** - just electricity
- **Direct access** - fastest possible
- **Easy updates** - just edit local files

‚ùå **CONS:**
- **No 24/7** - offline when PC off
- **ngrok hassle** - URL changes, Slack config pain
- **Single machine** - no redundancy
- **Limited scale** - your PC only

**Cost:** $2-10/month (electricity + ngrok)

**Setup:**
```bash
# 1. Organize training data locally
mkdir -p ~/bobs-training-data
cp -r ~/Documents/work ~/bobs-training-data/
cp -r ~/Documents/personal ~/bobs-training-data/

# 2. Configure Bob to read local data
cat >> .env <<EOF
KNOWLEDGE_DIR=/home/jeremy/bobs-training-data
TRAINING_DATA_PATH=/home/jeremy/Documents
EOF

# 3. Ingest into knowledge DB
cd ~/projects/bobs-brain
python scripts/ingest_knowledge.py ~/bobs-training-data

# 4. Start Bob locally
python -m flask --app src.app run --port 8080

# 5. Expose with ngrok
ngrok http 8080
```

---

### Option 3: Hybrid - General in Cloud, Personal Local üîÄ

**Architecture:**
```
Cloud Bob (Production)
    ‚îú‚îÄ‚îÄ General knowledge (public docs)
    ‚îú‚îÄ‚îÄ Slack integration
    ‚îú‚îÄ‚îÄ 24/7 availability
    ‚îî‚îÄ‚îÄ API: /api/query (X-API-Key required)

         +

Local Bob (Personal Knowledge)
    ‚îú‚îÄ‚îÄ Your personal data
    ‚îú‚îÄ‚îÄ Private documents
    ‚îú‚îÄ‚îÄ Development/testing
    ‚îî‚îÄ‚îÄ API: localhost:8080/api/query

Strategy:
    ‚îú‚îÄ‚îÄ Team queries ‚Üí Cloud Bob
    ‚îú‚îÄ‚îÄ Personal queries ‚Üí Local Bob (when PC on)
    ‚îî‚îÄ‚îÄ Fallback: Cloud Bob says "ask local Bob for personal stuff"
```

**How it works:**
1. **Cloud Bob:** General knowledge, handles most Slack requests
2. **Local Bob:** Personal data, you query directly when needed
3. **Clear separation:** Public vs private knowledge
4. **Best of both:** 24/7 availability + privacy

‚úÖ **PROS:**
- Privacy preserved (personal data stays local)
- 24/7 Slack for general queries
- Fast local access for personal stuff
- Reasonable cost ($5-15/month)

‚ùå **CONS:**
- Two separate Bobs (some confusion)
- Personal queries only work when PC on
- Must remember which Bob to ask

**Cost:** $5-15/month (cloud only, local is free)

---

### Option 4: Local Primary + Cloud Sync üîÑ

**Architecture:**
```
Local Bob (Primary)
    ‚îú‚îÄ‚îÄ All training data
    ‚îú‚îÄ‚îÄ Full knowledge base
    ‚îú‚îÄ‚îÄ Learning from corrections
    ‚îî‚îÄ‚îÄ Exports knowledge snapshots
         ‚Üì
    [Sync Script - Daily/Weekly]
         ‚Üì
Cloud Bob (Replica)
    ‚îú‚îÄ‚îÄ Receives knowledge exports
    ‚îú‚îÄ‚îÄ Loads into memory
    ‚îú‚îÄ‚îÄ Serves Slack 24/7
    ‚îî‚îÄ‚îÄ Slightly stale (1 day behind)
```

**How it works:**
1. **Train locally:** All avatar data on your machine
2. **Bob learns:** Circle of Life updates knowledge
3. **Export daily:** Script exports knowledge base
4. **Upload to cloud:** Encrypted upload to Cloud Storage
5. **Cloud loads on startup:** Reads latest snapshot

**Sync script:**
```bash
#!/bin/bash
# sync-knowledge-to-cloud.sh
# Run daily via cron

set -e

echo "Exporting local knowledge..."
cd ~/projects/bobs-brain

# Export SQLite knowledge DB
sqlite3 bb.db ".backup /tmp/bb_snapshot.db"

# Export Chroma embeddings
tar czf /tmp/chroma_snapshot.tar.gz .chroma/

# Encrypt before upload (for extra security)
gpg --encrypt --recipient bob@bobs-house-ai \
  /tmp/bb_snapshot.db \
  /tmp/chroma_snapshot.tar.gz

# Upload to cloud
gsutil cp /tmp/bb_snapshot.db.gpg gs://bobs-brain-knowledge/
gsutil cp /tmp/chroma_snapshot.tar.gz.gpg gs://bobs-brain-knowledge/

# Trigger cloud restart to load new data
gcloud run services update bobs-brain \
  --region us-central1 \
  --update-env-vars KNOWLEDGE_VERSION=$(date +%Y%m%d)

echo "‚úÖ Knowledge synced to cloud"
```

**Cron setup:**
```bash
# Edit crontab
crontab -e

# Add daily sync at 3am
0 3 * * * /home/jeremy/projects/bobs-brain/scripts/sync-knowledge-to-cloud.sh
```

‚úÖ **PROS:**
- Best of both worlds
- Privacy maintained (data local first)
- Cloud Bob stays updated
- Can audit what goes to cloud
- Encrypted uploads

‚ùå **CONS:**
- Cloud data slightly stale (1 day behind)
- Sync complexity
- Must manage encryption keys
- Cloud storage costs ($5-15/month)

**Cost:** $10-25/month (cloud + storage)

---

### Option 5: Cloud Bob Calls Local API üåê ‚Üê üíª

**Architecture:**
```
Cloud Bob (Slack Handler)
    ‚îú‚îÄ‚îÄ Receives Slack message
    ‚îú‚îÄ‚îÄ Determines if needs personal knowledge
    ‚îî‚îÄ‚îÄ Calls: https://jeremy-home.dyndns.org/bob-api
         ‚Üì
    [Internet / VPN]
         ‚Üì
Local Bob API (Your PC)
    ‚îú‚îÄ‚îÄ Secure API endpoint
    ‚îú‚îÄ‚îÄ Requires API key
    ‚îú‚îÄ‚îÄ Has full personal knowledge
    ‚îî‚îÄ‚îÄ Returns answer
         ‚Üì
Cloud Bob
    ‚îî‚îÄ‚îÄ Sends answer to Slack
```

**How it works:**
1. **Cloud Bob:** Lightweight, handles Slack
2. **Personal queries:** Forwards to your local Bob API
3. **Local Bob:** Responds with personal knowledge
4. **Cloud relays:** Sends answer back to Slack

**Requirements:**
- **Static IP or DynamicDNS** - Cloud needs to reach your PC
- **Port forward** - Router must allow incoming
- **API key auth** - Secure the endpoint
- **Your PC must be on** - Falls back if offline

‚úÖ **PROS:**
- Personal data never uploaded
- Cloud handles Slack 24/7
- Local only accessed when needed
- Clear separation

‚ùå **CONS:**
- Network complexity (port forward, DNS)
- Your PC must be on for personal queries
- Latency (cloud ‚Üí internet ‚Üí your home ‚Üí back)
- Security concerns (exposed endpoint)

**Setup:**
```bash
# 1. Local Bob exposes API
# Edit .env
echo "API_MODE=server" >> .env
echo "API_KEY=$(openssl rand -hex 32)" >> .env

# Start Bob API
python -m flask --app src.app run --host 0.0.0.0 --port 8080

# 2. Configure router port forwarding
# Forward external port 8443 ‚Üí your PC 8080

# 3. Set up DynamicDNS (e.g., noip.com)
# jeremy-home.ddns.net ‚Üí your dynamic IP

# 4. Cloud Bob configured to call you
# Cloud Run env var:
# PERSONAL_KNOWLEDGE_API=https://jeremy-home.ddns.net:8443
# PERSONAL_API_KEY=<your-api-key>
```

---

## Recommendation Matrix

| Your Priority | Best Option | Cost/Month | Complexity |
|---------------|-------------|------------|------------|
| **Maximum privacy** | Local only (Option 2) | $2-10 | Low |
| **24/7 Slack** | Hybrid (Option 3) | $5-15 | Medium |
| **Best of both** | Local + Sync (Option 4) | $10-25 | High |
| **Professional** | Everything Cloud (Option 1) | $26-70 | Medium |
| **Advanced** | Cloud calls Local (Option 5) | $5-15 | Very High |

---

## Recommended: Option 4 (Local + Cloud Sync)

**For you specifically, Jeremy:**

‚úÖ **Train locally** - All personal data on your PC
‚úÖ **Export daily** - Sync knowledge to cloud automatically
‚úÖ **Cloud serves Slack** - 24/7 availability
‚úÖ **Privacy preserved** - You control what syncs
‚úÖ **Development speed** - Fast local iteration

### Implementation Plan

**Phase 1: Set Up Local Training (Week 1)**
```bash
# 1. Organize training data
mkdir -p ~/bobs-training-data/{documents,code,personal,work}

# 2. Copy your avatar data
cp -r ~/Documents/resume ~/bobs-training-data/personal/
cp -r ~/Documents/projects ~/bobs-training-data/work/
cp -r ~/Documents/notes ~/bobs-training-data/personal/

# 3. Ingest into Bob's knowledge DB
cd ~/projects/bobs-brain
python scripts/ingest_knowledge.py ~/bobs-training-data

# 4. Verify ingestion
sqlite3 bb.db "SELECT COUNT(*) FROM knowledge"
# Should show 77,264+ documents
```

**Phase 2: Train Circle of Life (Week 2)**
```bash
# Start local Bob
python -m flask --app src.app run --port 8080

# Test and correct Bob
curl -X POST http://localhost:8080/api/query \
  -H "Content-Type: application/json" \
  -d '{"query":"Tell me about my work experience"}'

# Submit corrections
curl -X POST http://localhost:8080/learn \
  -H "Content-Type: application/json" \
  -d '{
    "correction": "When discussing work, mention 15 years in tech",
    "context": "User bio and experience"
  }'
```

**Phase 3: Set Up Cloud Sync (Week 3)**
```bash
# 1. Create sync script
nano scripts/sync-knowledge-to-cloud.sh
# (Use script from Option 4 above)

# 2. Make executable
chmod +x scripts/sync-knowledge-to-cloud.sh

# 3. Test manual sync
./scripts/sync-knowledge-to-cloud.sh

# 4. Set up daily cron
crontab -e
# Add: 0 3 * * * /home/jeremy/projects/bobs-brain/scripts/sync-knowledge-to-cloud.sh
```

**Phase 4: Deploy Cloud Bob (Week 4)**
```bash
# Deploy with knowledge loading
./05-Scripts/deploy/deploy-to-cloudrun.sh

# Cloud Bob downloads knowledge on startup from Cloud Storage
# Loads into memory, serves Slack
```

---

## Security Best Practices

### Encrypting Personal Data

**Option A: GPG Encryption**
```bash
# Encrypt before upload
gpg --encrypt --recipient bob@bobs-house-ai knowledge.db

# Cloud decrypts on load (requires private key)
```

**Option B: Google KMS**
```bash
# Encrypt with Cloud KMS
gcloud kms encrypt \
  --key bob-knowledge-key \
  --keyring bob-keyring \
  --location us-central1 \
  --plaintext-file knowledge.db \
  --ciphertext-file knowledge.db.enc

# Upload encrypted
gsutil cp knowledge.db.enc gs://bobs-brain-knowledge/

# Cloud Bob decrypts automatically (has KMS permissions)
```

### Access Controls

**Minimal permissions:**
```yaml
Cloud Storage Bucket:
  - Bob's service account: Read only
  - Your account: Read/Write
  - Everyone else: No access

Secret Manager:
  - Bob's service account: Access secrets
  - Audit logs: Enabled

Cloud Run:
  - Service account: bob-brain@bobs-house-ai.iam.gserviceaccount.com
  - VPC: private network (optional)
  - Ingress: Slack webhook only
```

### Audit What Gets Synced

**Pre-sync filter:**
```python
# scripts/filter_knowledge.py
"""
Filter personal data before cloud sync
"""

EXCLUDE_PATTERNS = [
    "**/private/**",
    "**/secret/**",
    "**/*_personal.md",
    "**/passwords/**",
]

def should_sync(filepath):
    """Return False if file should stay local"""
    for pattern in EXCLUDE_PATTERNS:
        if filepath.match(pattern):
            return False
    return True

# Use in sync script
for file in knowledge_files:
    if should_sync(file):
        upload_to_cloud(file)
    else:
        print(f"Skipping private file: {file}")
```

---

## Data Access Patterns

### Where Cloud Bob Gets Data

```
Cloud Bob Memory (Runtime):
‚îú‚îÄ‚îÄ Startup: Load from Cloud Storage
‚îÇ   ‚îú‚îÄ‚îÄ knowledge.db (SQLite)
‚îÇ   ‚îú‚îÄ‚îÄ chroma/ (vector embeddings)
‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ During conversation:
‚îÇ   ‚îú‚îÄ‚îÄ Query knowledge DB (in memory)
‚îÇ   ‚îú‚îÄ‚îÄ Search Chroma vectors
‚îÇ   ‚îî‚îÄ‚îÄ Call Gemini API
‚îî‚îÄ‚îÄ Learning:
    ‚îî‚îÄ‚îÄ Store in Cloud SQL (persistent)

Cloud Bob doesn't:
‚ùå Access your local filesystem
‚ùå Connect to your PC
‚ùå Read from ~/Documents
‚ùå Access local SQLite directly
```

### Where Local Bob Gets Data

```
Local Bob (Your PC):
‚îú‚îÄ‚îÄ Direct filesystem access:
‚îÇ   ‚îú‚îÄ‚îÄ ~/bobs-training-data/
‚îÇ   ‚îú‚îÄ‚îÄ ~/Documents/
‚îÇ   ‚îú‚îÄ‚îÄ ~/projects/
‚îÇ   ‚îî‚îÄ‚îÄ ~/research/
‚îú‚îÄ‚îÄ Local databases:
‚îÇ   ‚îú‚îÄ‚îÄ ./bb.db (SQLite)
‚îÇ   ‚îú‚îÄ‚îÄ ./.chroma/ (vectors)
‚îÇ   ‚îî‚îÄ‚îÄ ./artifacts/
‚îî‚îÄ‚îÄ Learning:
    ‚îî‚îÄ‚îÄ Persists to local SQLite

Local Bob can:
‚úÖ Read any file on your PC
‚úÖ Access local databases
‚úÖ Run scripts to gather data
‚úÖ Monitor filesystem changes
```

---

## Next Steps

**Immediate (Today):**
1. Decide which option fits your needs
2. Organize training data locally
3. Test ingestion into local Bob

**This Week:**
1. Train local Bob with your avatar data
2. Submit corrections to Circle of Life
3. Verify knowledge base is accurate

**Next Week:**
1. Set up sync script (if using Option 4)
2. Deploy cloud Bob
3. Configure Slack

**Ongoing:**
1. Train Bob with new documents
2. Submit corrections as needed
3. Monitor cloud sync logs

---

## FAQ

**Q: How much personal data can Bob handle?**
A: Current setup: 77,264 documents. Can scale to millions with proper indexing.

**Q: Will cloud Bob have real-time access to my data?**
A: Option 1 (yes), Option 4 (1-day lag), Option 5 (yes but requires PC on)

**Q: Can I delete data from cloud later?**
A: Yes, delete from Cloud Storage bucket. Bob loads on startup.

**Q: What if I don't want ANY data in cloud?**
A: Use Option 2 (local only) or Option 3 (hybrid, cloud gets general knowledge only)

**Q: How do I update cloud Bob's knowledge?**
A: Re-run sync script, or manually upload to Cloud Storage and restart service

**Q: Can cloud Bob learn from Slack conversations?**
A: Yes, Circle of Life can store learnings in Cloud SQL (persistent)

---

## Cost Summary

| Option | Monthly Cost | Setup Time | Privacy | 24/7 Access |
|--------|--------------|------------|---------|-------------|
| **1. Everything Cloud** | $26-70 | 1 hour | Low | ‚úÖ |
| **2. Everything Local** | $2-10 | 30 min | ‚úÖ High | ‚ùå |
| **3. Hybrid** | $5-15 | 1 hour | ‚úÖ High | Partial |
| **4. Local + Sync** | $10-25 | 2 hours | Medium | ‚úÖ |
| **5. Cloud Calls Local** | $5-15 | 3 hours | ‚úÖ High | Partial |

**Recommended: Option 4** - $10-25/month, best balance

---

**Created:** 2025-10-05
**Status:** ‚úÖ Complete analysis
**Recommendation:** Local training + daily sync to cloud
**Next:** Organize training data, start local ingestion

