#!/usr/bin/env python3
"""
Bob's Brain - 100% Google Cloud Native
No OpenAI, No external dependencies - Pure GCP
"""

import os
import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, List

from flask import Flask, request, jsonify
from slack_sdk import WebClient

# GOOGLE CLOUD NATIVE SERVICES ONLY
from google.cloud import firestore
from google.cloud import bigquery
from google.cloud import aiplatform
import google.generativeai as genai
import vertexai
from vertexai.generative_models import GenerativeModel
from vertexai.language_models import TextEmbeddingModel

# NO GRAPHITI - Use Google's Knowledge Graph API instead
from google.cloud import enterpriseknowledgegraph as ekg

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

class BobGCPNative:
    """
    Bob using ONLY Google Cloud Platform services:
    - Vertex AI for LLM (Gemini)
    - Vertex AI for embeddings
    - BigQuery for ML models
    - Firestore for real-time data
    - Enterprise Knowledge Graph (instead of Neo4j/Graphiti)
    """
    
    def __init__(self):
        logger.info("ðŸš€ Initializing Bob with 100% Google Cloud services...")
        
        # Initialize GCP project
        self.project_id = 'bobs-house-ai'
        self.location = 'us-central1'
        
        # 1. VERTEX AI - For LLM and embeddings
        vertexai.init(project=self.project_id, location=self.location)
        
        # Gemini for chat/responses
        self.gemini = GenerativeModel('gemini-1.5-flash')
        self.gemini_pro = GenerativeModel('gemini-1.5-pro')
        logger.info("âœ… Vertex AI Gemini models initialized")
        
        # Text embeddings for semantic search
        self.embeddings_model = TextEmbeddingModel.from_pretrained("textembedding-gecko@003")
        logger.info("âœ… Vertex AI embeddings model initialized")
        
        # 2. BIGQUERY - For ML models and analytics
        self.bigquery = bigquery.Client(project=self.project_id)
        self._ensure_ml_models_exist()
        logger.info("âœ… BigQuery ML connected")
        
        # 3. FIRESTORE - For real-time data
        self.firestore = firestore.Client(
            project='diagnostic-pro-mvp',
            database='bob-brain'
        )
        logger.info("âœ… Firestore connected")
        
        # 4. AUTOML - For advanced ML models
        aiplatform.init(project=self.project_id, location=self.location)
        self.automl_ready = True
        logger.info("âœ… AutoML ready for training")
        
        # 5. SLACK - For communication
        self.slack = WebClient(token=os.environ.get('SLACK_BOT_TOKEN'))
        logger.info("âœ… Slack client connected")
        
        logger.info("ðŸŽ¯ Bob initialized with 100% Google Cloud services!")
    
    def _ensure_ml_models_exist(self):
        """Create BigQuery ML models if they don't exist"""
        
        # Check if dataset exists
        dataset_id = f"{self.project_id}.ml_models"
        dataset = bigquery.Dataset(dataset_id)
        dataset.location = "US"
        
        try:
            self.bigquery.create_dataset(dataset, exists_ok=True)
            logger.info("âœ… ML models dataset ready")
        except Exception as e:
            logger.warning(f"Dataset exists or error: {e}")
        
        # Create price prediction model if not exists
        try:
            query = f"""
            CREATE MODEL IF NOT EXISTS `{self.project_id}.ml_models.price_predictor`
            OPTIONS(
              model_type='linear_reg',
              input_label_cols=['quoted_price']
            ) AS
            SELECT 
              IFNULL(vehicle_make, 'unknown') as vehicle_make,
              IFNULL(vehicle_model, 'unknown') as vehicle_model,
              IFNULL(vehicle_year, 2020) as vehicle_year,
              IFNULL(repair_type, 'general') as repair_type,
              quoted_price
            FROM `{self.project_id}.scraped_data.repair_quotes`
            WHERE quoted_price IS NOT NULL
            LIMIT 100
            """
            self.bigquery.query(query).result()
            logger.info("âœ… Price prediction model ready")
        except Exception as e:
            logger.info(f"Model exists or no data yet: {e}")
    
    async def process_message(self, text: str, user: str, channel: str) -> str:
        """
        Process message using Google Cloud services:
        1. Generate embeddings with Vertex AI
        2. Search Firestore with embeddings
        3. Get ML predictions from BigQuery
        4. Generate response with Gemini
        """
        
        try:
            # 1. Generate embeddings for the query
            query_embedding = await self._get_embedding(text)
            
            # 2. Semantic search in Firestore
            similar_docs = await self._semantic_search(query_embedding)
            
            # 3. Get ML predictions from BigQuery
            ml_predictions = await self._get_ml_predictions(text)
            
            # 4. Get recent data from Firestore
            recent_data = await self._get_recent_data()
            
            # 5. Generate response with Gemini
            context = self._build_context(similar_docs, ml_predictions, recent_data)
            response = await self._generate_response(text, context)
            
            # 6. Store conversation
            await self._store_conversation(text, response, user, channel)
            
            return response
            
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            return self._fallback_response(text)
    
    async def _get_embedding(self, text: str) -> List[float]:
        """Generate embeddings using Vertex AI"""
        try:
            embeddings = self.embeddings_model.get_embeddings([text])
            return embeddings[0].values
        except Exception as e:
            logger.error(f"Embedding error: {e}")
            return [0.0] * 768  # Default embedding size
    
    async def _semantic_search(self, embedding: List[float], limit: int = 5) -> List[Dict]:
        """Search Firestore using embeddings (simplified)"""
        # For now, return recent documents
        # In production, you'd use Vertex AI Matching Engine for vector search
        docs = []
        try:
            results = self.firestore.collection('knowledge').limit(limit).stream()
            for doc in results:
                docs.append(doc.to_dict())
        except Exception as e:
            logger.error(f"Search error: {e}")
        return docs
    
    async def _get_ml_predictions(self, text: str) -> Dict[str, Any]:
        """Get predictions from BigQuery ML models"""
        predictions = {}
        
        try:
            # Extract repair type from text using Gemini
            extract_prompt = f"Extract the repair type from this text. Return ONLY the repair type or 'unknown': {text}"
            response = self.gemini.generate_content(extract_prompt)
            repair_type = response.text.strip()
            
            if repair_type != 'unknown':
                # Get price prediction from BigQuery ML
                query = f"""
                SELECT 
                  predicted_quoted_price as predicted_price
                FROM ML.PREDICT(
                  MODEL `{self.project_id}.ml_models.price_predictor`,
                  (
                    SELECT 
                      'Toyota' as vehicle_make,
                      'Camry' as vehicle_model,
                      2020 as vehicle_year,
                      '{repair_type}' as repair_type
                  )
                )
                """
                
                result = self.bigquery.query(query).result()
                for row in result:
                    predictions['predicted_price'] = row.predicted_price
                    break
                
                # Also get average from historical data
                avg_query = f"""
                SELECT 
                  AVG(quoted_price) as avg_price,
                  MIN(quoted_price) as min_price,
                  MAX(quoted_price) as max_price,
                  COUNT(*) as sample_size
                FROM `{self.project_id}.scraped_data.repair_quotes`
                WHERE LOWER(repair_type) LIKE LOWER('%{repair_type}%')
                """
                
                avg_result = self.bigquery.query(avg_query).result()
                for row in avg_result:
                    predictions['historical_avg'] = row.avg_price
                    predictions['min_price'] = row.min_price
                    predictions['max_price'] = row.max_price
                    predictions['sample_size'] = row.sample_size
                    
        except Exception as e:
            logger.error(f"ML prediction error: {e}")
            
        return predictions
    
    async def _get_recent_data(self) -> List[Dict]:
        """Get recent data from Firestore"""
        recent = []
        try:
            docs = self.firestore.collection('diagnostic_submissions').order_by(
                'timestamp', direction=firestore.Query.DESCENDING
            ).limit(3).stream()
            
            for doc in docs:
                recent.append(doc.to_dict())
        except Exception as e:
            logger.error(f"Recent data error: {e}")
            
        return recent
    
    def _build_context(self, similar_docs: List[Dict], ml_predictions: Dict, recent_data: List[Dict]) -> str:
        """Build context for Gemini"""
        context = "Context for response:\n\n"
        
        if similar_docs:
            context += "Related Knowledge:\n"
            for doc in similar_docs[:3]:
                context += f"- {doc.get('content', str(doc))[:200]}\n"
            context += "\n"
        
        if ml_predictions:
            context += "ML Predictions:\n"
            if 'predicted_price' in ml_predictions:
                context += f"- Model predicted price: ${ml_predictions['predicted_price']:.2f}\n"
            if 'historical_avg' in ml_predictions:
                context += f"- Historical average: ${ml_predictions['historical_avg']:.2f}\n"
                context += f"- Price range: ${ml_predictions['min_price']:.2f} - ${ml_predictions['max_price']:.2f}\n"
                context += f"- Based on {ml_predictions['sample_size']} samples\n"
            context += "\n"
        
        if recent_data:
            context += "Recent Activity:\n"
            for data in recent_data:
                context += f"- {data.get('vehicle_make', '')} {data.get('repair_type', '')}: ${data.get('quoted_price', 0)}\n"
        
        return context
    
    async def _generate_response(self, question: str, context: str) -> str:
        """Generate response using Gemini"""
        prompt = f"""
        You are Bob, an AI assistant for DiagnosticPro.
        
        {context}
        
        User Question: {question}
        
        Provide a helpful, accurate response using the context provided.
        If ML predictions are available, mention them.
        Be conversational and helpful.
        
        Bob:
        """
        
        response = self.gemini.generate_content(prompt)
        return response.text
    
    async def _store_conversation(self, question: str, response: str, user: str, channel: str):
        """Store conversation in Firestore"""
        try:
            self.firestore.collection('bob_conversations').add({
                'user': user,
                'channel': channel,
                'question': question,
                'response': response,
                'timestamp': datetime.now(),
                'model': 'gemini-1.5-flash',
                'platform': 'gcp-native'
            })
        except Exception as e:
            logger.error(f"Failed to store conversation: {e}")
    
    def _fallback_response(self, text: str) -> str:
        """Simple fallback using Gemini"""
        try:
            prompt = f"You are Bob, a helpful AI assistant. User asked: {text}. Provide a brief helpful response."
            response = self.gemini.generate_content(prompt)
            return response.text
        except:
            return "I'm having trouble processing that right now. Please try again."
    
    async def train_automl_model(self, dataset_name: str, target_column: str):
        """Train an AutoML model using Vertex AI"""
        try:
            # Create AutoML training job
            job = aiplatform.AutoMLTabularTrainingJob(
                display_name=f"automl_{dataset_name}_{datetime.now().strftime('%Y%m%d')}",
                optimization_prediction_type="regression" if "price" in target_column else "classification",
                optimization_objective="minimize-rmse" if "price" in target_column else "maximize-au-prc"
            )
            
            # Start training
            model = job.run(
                dataset=f"bq://{self.project_id}.scraped_data.{dataset_name}",
                target_column=target_column,
                training_fraction_split=0.8,
                validation_fraction_split=0.1,
                test_fraction_split=0.1,
                budget_milli_node_hours=1000,  # Uses credits
                model_display_name=f"{dataset_name}_automl_model"
            )
            
            logger.info(f"âœ… AutoML model training started: {model.resource_name}")
            return model
            
        except Exception as e:
            logger.error(f"AutoML training error: {e}")
            return None

# Initialize Bob with GCP-only services
bob = BobGCPNative()

@app.route('/health', methods=['GET'])
def health():
    """Health check showing GCP services"""
    return jsonify({
        'status': 'healthy',
        'service': 'Bob GCP Native',
        'platform': '100% Google Cloud',
        'services': {
            'llm': 'Vertex AI Gemini',
            'embeddings': 'Vertex AI Text Embeddings',
            'ml_models': 'BigQuery ML',
            'automl': 'Vertex AI AutoML',
            'database': 'Firestore',
            'analytics': 'BigQuery'
        },
        'no_external_deps': True
    })

@app.route('/slack/events', methods=['POST'])
def slack_events():
    """Handle Slack events"""
    try:
        data = request.json
        
        # URL verification
        if data.get('type') == 'url_verification':
            return jsonify({'challenge': data['challenge']})
        
        # Process messages
        if data.get('type') == 'event_callback':
            event = data.get('event', {})
            
            if event.get('type') == 'message' and not event.get('bot_id'):
                user = event.get('user')
                text = event.get('text')
                channel = event.get('channel')
                
                # Process with GCP services
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                response = loop.run_until_complete(
                    bob.process_message(text, user, channel)
                )
                
                # Send response
                bob.slack.chat_postMessage(
                    channel=channel,
                    text=response
                )
        
        return jsonify({'status': 'ok'})
        
    except Exception as e:
        logger.error(f"Slack error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/train-automl', methods=['POST'])
def train_automl():
    """Train an AutoML model"""
    try:
        data = request.json
        dataset = data.get('dataset', 'repair_quotes')
        target = data.get('target', 'quoted_price')
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        model = loop.run_until_complete(
            bob.train_automl_model(dataset, target)
        )
        
        if model:
            return jsonify({
                'status': 'training_started',
                'model': str(model.resource_name),
                'message': 'AutoML training started. Check Vertex AI console for progress.'
            })
        else:
            return jsonify({'error': 'Failed to start training'}), 500
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/predict', methods=['POST'])
def predict():
    """Get ML predictions"""
    try:
        data = request.json
        text = data.get('text', '')
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        predictions = loop.run_until_complete(
            bob._get_ml_predictions(text)
        )
        
        return jsonify(predictions)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/', methods=['GET'])
def index():
    """API documentation"""
    return jsonify({
        'service': 'Bob GCP Native',
        'version': '7.0',
        'description': '100% Google Cloud Platform implementation',
        'frameworks': {
            'vertex_ai': ['Gemini 1.5', 'Text Embeddings', 'AutoML'],
            'bigquery_ml': ['Linear Regression', 'Logistic Regression', 'K-Means'],
            'firestore': 'NoSQL Database',
            'bigquery': 'Data Warehouse & ML'
        },
        'endpoints': {
            '/health': 'System status',
            '/slack/events': 'Slack integration',
            '/train-automl': 'Train AutoML model',
            '/predict': 'Get ML predictions'
        },
        'credits_usage': {
            'gemini': '$0.0001 per 1K chars',
            'embeddings': '$0.00002 per 1K chars',
            'bigquery_ml': '$5 per TB processed',
            'automl': '$20 per model',
            'total_available': '$2,251.82'
        }
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)