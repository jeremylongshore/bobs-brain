#!/usr/bin/env python3
"""
Bob's Brain - 100% Google Cloud Native
No OpenAI, No external dependencies - Pure GCP
"""

import asyncio
import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List

import google.generativeai as genai
import vertexai
from flask import Flask, jsonify, request

# NO GRAPHITI - Use Google's Knowledge Graph API instead
# GOOGLE CLOUD NATIVE SERVICES ONLY
from google.cloud import aiplatform, bigquery
from google.cloud import enterpriseknowledgegraph as ekg
from google.cloud import firestore
from slack_sdk import WebClient
from vertexai.generative_models import GenerativeModel
from vertexai.language_models import TextEmbeddingModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)


class BobGCPNative:
    """
    Bob using ONLY Google Cloud Platform services:
    - Vertex AI for LLM (Gemini)
    - Vertex AI for embeddings
    - BigQuery for ML models
    - Firestore for real-time data
    - Enterprise Knowledge Graph (instead of Neo4j/Graphiti)
    """

    def __init__(self):
        logger.info("ðŸš€ Initializing Bob with 100% Google Cloud services...")

        # Initialize GCP project
        self.project_id = "bobs-house-ai"
        self.location = "us-central1"

        # 1. VERTEX AI - For LLM and embeddings
        vertexai.init(project=self.project_id, location=self.location)

        # Gemini for chat/responses
        self.gemini = GenerativeModel("gemini-1.5-flash")
        self.gemini_pro = GenerativeModel("gemini-1.5-pro")
        logger.info("âœ… Vertex AI Gemini models initialized")

        # Text embeddings for semantic search
        self.embeddings_model = TextEmbeddingModel.from_pretrained("textembedding-gecko@003")
        logger.info("âœ… Vertex AI embeddings model initialized")

        # 2. BIGQUERY - For ML models and analytics
        self.bigquery = bigquery.Client(project=self.project_id)
        self._ensure_ml_models_exist()
        logger.info("âœ… BigQuery ML connected")

        # 3. FIRESTORE - For real-time data
        self.firestore = firestore.Client(project="diagnostic-pro-mvp", database="bob-brain")
        logger.info("âœ… Firestore connected")

        # 4. AUTOML - For advanced ML models
        aiplatform.init(project=self.project_id, location=self.location)
        self.automl_ready = True
        logger.info("âœ… AutoML ready for training")

        # 5. SLACK - For communication
        self.slack = WebClient(token=os.environ.get("SLACK_BOT_TOKEN"))
        logger.info("âœ… Slack client connected")

        logger.info("ðŸŽ¯ Bob initialized with 100% Google Cloud services!")

    def _ensure_ml_models_exist(self):
        """Create BigQuery ML models if they don't exist"""

        # Check if dataset exists
        dataset_id = f"{self.project_id}.ml_models"
        dataset = bigquery.Dataset(dataset_id)
        dataset.location = "US"

        try:
            self.bigquery.create_dataset(dataset, exists_ok=True)
            logger.info("âœ… ML models dataset ready")
        except Exception as e:
            logger.warning(f"Dataset exists or error: {e}")

        # Create price prediction model if not exists
        try:
            query = f"""
            CREATE MODEL IF NOT EXISTS `{self.project_id}.ml_models.price_predictor`
            OPTIONS(
              model_type='linear_reg',
              input_label_cols=['quoted_price']
            ) AS
            SELECT
              IFNULL(vehicle_make, 'unknown') as vehicle_make,
              IFNULL(vehicle_model, 'unknown') as vehicle_model,
              IFNULL(vehicle_year, 2020) as vehicle_year,
              IFNULL(repair_type, 'general') as repair_type,
              quoted_price
            FROM `{self.project_id}.scraped_data.repair_quotes`
            WHERE quoted_price IS NOT NULL
            LIMIT 100
            """
            self.bigquery.query(query).result()
            logger.info("âœ… Price prediction model ready")
        except Exception as e:
            logger.info(f"Model exists or no data yet: {e}")

    async def process_message(self, text: str, user: str, channel: str) -> str:
        """
        Process message using Google Cloud services:
        1. Generate embeddings with Vertex AI
        2. Search Firestore with embeddings
        3. Get ML predictions from BigQuery
        4. Generate response with Gemini
        """

        try:
            # 1. Generate embeddings for the query
            query_embedding = await self._get_embedding(text)

            # 2. Semantic search in Firestore
            similar_docs = await self._semantic_search(query_embedding)

            # 3. Get ML predictions from BigQuery
            ml_predictions = await self._get_ml_predictions(text)

            # 4. Get recent data from Firestore
            recent_data = await self._get_recent_data()

            # 5. Generate response with Gemini
            context = self._build_context(similar_docs, ml_predictions, recent_data)
            response = await self._generate_response(text, context)

            # 6. Store conversation
            await self._store_conversation(text, response, user, channel)

            return response

        except Exception as e:
            logger.error(f"Error processing message: {e}")
            return self._fallback_response(text)

    async def _get_embedding(self, text: str) -> List[float]:
        """Generate embeddings using Vertex AI"""
        try:
            embeddings = self.embeddings_model.get_embeddings([text])
            return embeddings[0].values
        except Exception as e:
            logger.error(f"Embedding error: {e}")
            return [0.0] * 768  # Default embedding size

    async def _semantic_search(self, embedding: List[float], limit: int = 5) -> List[Dict]:
        """Search Firestore using embeddings (simplified)"""
        # For now, return recent documents
        # In production, you'd use Vertex AI Matching Engine for vector search
        docs = []
        try:
            results = self.firestore.collection("knowledge").limit(limit).stream()
            for doc in results:
                docs.append(doc.to_dict())
        except Exception as e:
            logger.error(f"Search error: {e}")
        return docs

    async def _get_ml_predictions(self, text: str) -> Dict[str, Any]:
        """Get predictions from BigQuery ML models"""
        predictions = {}

        try:
            # Extract repair type from text using Gemini
            extract_prompt = f"Extract the repair type from this text. Return ONLY the repair type or 'unknown': {text}"
            response = self.gemini.generate_content(extract_prompt)
            repair_type = response.text.strip()

            if repair_type != "unknown":
                # Get price prediction from BigQuery ML
                query = f"""
                SELECT
                  predicted_quoted_price as predicted_price
                FROM ML.PREDICT(
                  MODEL `{self.project_id}.ml_models.price_predictor`,
                  (
                    SELECT
                      'Toyota' as vehicle_make,
                      'Camry' as vehicle_model,
                      2020 as vehicle_year,
                      '{repair_type}' as repair_type
                  )
                )
                """

                result = self.bigquery.query(query).result()
                for row in result:
                    predictions["predicted_price"] = row.predicted_price
                    break

                # Also get average from historical data
                avg_query = f"""
                SELECT
                  AVG(quoted_price) as avg_price,
                  MIN(quoted_price) as min_price,
                  MAX(quoted_price) as max_price,
                  COUNT(*) as sample_size
                FROM `{self.project_id}.scraped_data.repair_quotes`
                WHERE LOWER(repair_type) LIKE LOWER('%{repair_type}%')
                """

                avg_result = self.bigquery.query(avg_query).result()
                for row in avg_result:
                    predictions["historical_avg"] = row.avg_price
                    predictions["min_price"] = row.min_price
                    predictions["max_price"] = row.max_price
                    predictions["sample_size"] = row.sample_size

        except Exception as e:
            logger.error(f"ML prediction error: {e}")

        return predictions

    async def _get_recent_data(self) -> List[Dict]:
        """Get recent data from Firestore"""
        recent = []
        try:
            docs = (
                self.firestore.collection("diagnostic_submissions")
                .order_by("timestamp", direction=firestore.Query.DESCENDING)
                .limit(3)
                .stream()
            )

            for doc in docs:
                recent.append(doc.to_dict())
        except Exception as e:
            logger.error(f"Recent data error: {e}")

        return recent

    def _build_context(self, similar_docs: List[Dict], ml_predictions: Dict, recent_data: List[Dict]) -> str:
        """Build context for Gemini"""
        context = "Context for response:\n\n"

        if similar_docs:
            context += "Related Knowledge:\n"
            for doc in similar_docs[:3]:
                context += f"- {doc.get('content', str(doc))[:200]}\n"
            context += "\n"

        if ml_predictions:
            context += "ML Predictions:\n"
            if "predicted_price" in ml_predictions:
                context += f"- Model predicted price: ${ml_predictions['predicted_price']:.2f}\n"
            if "historical_avg" in ml_predictions:
                context += f"- Historical average: ${ml_predictions['historical_avg']:.2f}\n"
                context += f"- Price range: ${ml_predictions['min_price']:.2f} - ${ml_predictions['max_price']:.2f}\n"
                context += f"- Based on {ml_predictions['sample_size']} samples\n"
            context += "\n"

        if recent_data:
            context += "Recent Activity:\n"
            for data in recent_data:
                context += (
                    f"- {data.get('vehicle_make', '')} {data.get('repair_type', '')}: ${data.get('quoted_price', 0)}\n"
                )

        return context

    async def _generate_response(self, question: str, context: str) -> str:
        """Generate response using Gemini"""
        prompt = f"""
        You are Bob, an AI assistant for DiagnosticPro.

        {context}

        User Question: {question}

        Provide a helpful, accurate response using the context provided.
        If ML predictions are available, mention them.
        Be conversational and helpful.

        Bob:
        """

        response = self.gemini.generate_content(prompt)
        return response.text

    async def _store_conversation(self, question: str, response: str, user: str, channel: str):
        """Store conversation in Firestore"""
        try:
            self.firestore.collection("bob_conversations").add(
                {
                    "user": user,
                    "channel": channel,
                    "question": question,
                    "response": response,
                    "timestamp": datetime.now(),
                    "model": "gemini-1.5-flash",
                    "platform": "gcp-native",
                }
            )
        except Exception as e:
            logger.error(f"Failed to store conversation: {e}")

    def _fallback_response(self, text: str) -> str:
        """Simple fallback using Gemini"""
        try:
            prompt = f"You are Bob, a helpful AI assistant. User asked: {text}. Provide a brief helpful response."
            response = self.gemini.generate_content(prompt)
            return response.text
        except:
            return "I'm having trouble processing that right now. Please try again."

    async def train_automl_model(self, dataset_name: str, target_column: str):
        """Train an AutoML model using Vertex AI"""
        try:
            # Create AutoML training job
            job = aiplatform.AutoMLTabularTrainingJob(
                display_name=f"automl_{dataset_name}_{datetime.now().strftime('%Y%m%d')}",
                optimization_prediction_type="regression" if "price" in target_column else "classification",
                optimization_objective="minimize-rmse" if "price" in target_column else "maximize-au-prc",
            )

            # Start training
            model = job.run(
                dataset=f"bq://{self.project_id}.scraped_data.{dataset_name}",
                target_column=target_column,
                training_fraction_split=0.8,
                validation_fraction_split=0.1,
                test_fraction_split=0.1,
                budget_milli_node_hours=1000,  # Uses credits
                model_display_name=f"{dataset_name}_automl_model",
            )

            logger.info(f"âœ… AutoML model training started: {model.resource_name}")
            return model

        except Exception as e:
            logger.error(f"AutoML training error: {e}")
            return None


# Initialize Bob with GCP-only services
bob = BobGCPNative()


@app.route("/health", methods=["GET"])
def health():
    """Health check showing GCP services"""
    return jsonify(
        {
            "status": "healthy",
            "service": "Bob GCP Native",
            "platform": "100% Google Cloud",
            "services": {
                "llm": "Vertex AI Gemini",
                "embeddings": "Vertex AI Text Embeddings",
                "ml_models": "BigQuery ML",
                "automl": "Vertex AI AutoML",
                "database": "Firestore",
                "analytics": "BigQuery",
            },
            "no_external_deps": True,
        }
    )


@app.route("/slack/events", methods=["POST"])
def slack_events():
    """Handle Slack events"""
    try:
        data = request.json

        # URL verification
        if data.get("type") == "url_verification":
            return jsonify({"challenge": data["challenge"]})

        # Process messages
        if data.get("type") == "event_callback":
            event = data.get("event", {})

            if event.get("type") == "message" and not event.get("bot_id"):
                user = event.get("user")
                text = event.get("text")
                channel = event.get("channel")

                # Process with GCP services
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                response = loop.run_until_complete(bob.process_message(text, user, channel))

                # Send response
                bob.slack.chat_postMessage(channel=channel, text=response)

        return jsonify({"status": "ok"})

    except Exception as e:
        logger.error(f"Slack error: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/train-automl", methods=["POST"])
def train_automl():
    """Train an AutoML model"""
    try:
        data = request.json
        dataset = data.get("dataset", "repair_quotes")
        target = data.get("target", "quoted_price")

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        model = loop.run_until_complete(bob.train_automl_model(dataset, target))

        if model:
            return jsonify(
                {
                    "status": "training_started",
                    "model": str(model.resource_name),
                    "message": "AutoML training started. Check Vertex AI console for progress.",
                }
            )
        else:
            return jsonify({"error": "Failed to start training"}), 500

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/predict", methods=["POST"])
def predict():
    """Get ML predictions"""
    try:
        data = request.json
        text = data.get("text", "")

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        predictions = loop.run_until_complete(bob._get_ml_predictions(text))

        return jsonify(predictions)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/", methods=["GET"])
def index():
    """API documentation"""
    return jsonify(
        {
            "service": "Bob GCP Native",
            "version": "7.0",
            "description": "100% Google Cloud Platform implementation",
            "frameworks": {
                "vertex_ai": ["Gemini 1.5", "Text Embeddings", "AutoML"],
                "bigquery_ml": ["Linear Regression", "Logistic Regression", "K-Means"],
                "firestore": "NoSQL Database",
                "bigquery": "Data Warehouse & ML",
            },
            "endpoints": {
                "/health": "System status",
                "/slack/events": "Slack integration",
                "/train-automl": "Train AutoML model",
                "/predict": "Get ML predictions",
            },
            "credits_usage": {
                "gemini": "$0.0001 per 1K chars",
                "embeddings": "$0.00002 per 1K chars",
                "bigquery_ml": "$5 per TB processed",
                "automl": "$20 per model",
                "total_available": "$2,251.82",
            },
        }
    )


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port)
