#!/usr/bin/env python3
"""
Master Test Runner for Bob Base Model
Runs all tests and generates comprehensive report
"""

import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path


class BobTestRunner:
    """Master test runner for all Bob components"""

    def __init__(self):
        self.project_root = Path(__file__).parent
        self.test_dir = self.project_root / "tests"
        self.report_dir = self.project_root / "test_reports"
        self.report_dir.mkdir(exist_ok=True)

        self.results = {"timestamp": datetime.now().isoformat(), "tests_run": [], "summary": {}, "details": {}}

    def run_test_file(self, test_file: str, description: str) -> dict:
        """Run a single test file and capture results"""
        print(f"\n{'='*60}")
        print(f"Running: {description}")
        print("=" * 60)

        result = {
            "file": test_file,
            "description": description,
            "status": "failed",
            "output": "",
            "error": "",
            "start_time": datetime.now().isoformat(),
        }

        try:
            # Run the test
            process = subprocess.run(
                [sys.executable, test_file], capture_output=True, text=True, cwd=self.project_root, timeout=60
            )

            result["output"] = process.stdout
            result["error"] = process.stderr
            result["return_code"] = process.returncode
            result["status"] = "passed" if process.returncode == 0 else "failed"

            # Print summary
            if process.returncode == 0:
                print(f"‚úÖ {description} - PASSED")
            else:
                print(f"‚ùå {description} - FAILED")
                if process.stderr:
                    print(f"   Error: {process.stderr[:200]}")

        except subprocess.TimeoutExpired:
            result["status"] = "timeout"
            result["error"] = "Test exceeded 60 second timeout"
            print(f"‚è±Ô∏è {description} - TIMEOUT")

        except Exception as e:
            result["status"] = "error"
            result["error"] = str(e)
            print(f"‚ùå {description} - ERROR: {e}")

        result["end_time"] = datetime.now().isoformat()
        return result

    def run_all_tests(self):
        """Run all test suites"""
        print(
            """
        ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
        ‚ïë   BOB BASE MODEL - MASTER TEST RUN   ‚ïë
        ‚ïë   Running All Test Suites            ‚ïë
        ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """
        )

        # Define test suites to run
        test_suites = [
            {"file": "tests/test_memory_only.py", "description": "Memory System Tests (Standalone)", "required": True},
            {"file": "test_migration.py", "description": "Firestore Migration Tests", "required": False},
            {
                "file": "tests/test_bob_base.py",
                "description": "Bob Base Model Tests (Full Suite)",
                "required": False,  # May fail without Slack tokens
            },
        ]

        # Run each test suite
        passed_count = 0
        failed_count = 0
        skipped_count = 0

        for suite in test_suites:
            test_path = self.project_root / suite["file"]

            if not test_path.exists():
                print(f"\n‚ö†Ô∏è Skipping {suite['description']} - File not found")
                skipped_count += 1
                continue

            result = self.run_test_file(str(test_path), suite["description"])
            self.results["tests_run"].append(result)

            if result["status"] == "passed":
                passed_count += 1
            else:
                failed_count += 1
                if suite["required"]:
                    print(f"   ‚ö†Ô∏è This was a required test!")

        # Calculate summary
        self.results["summary"] = {
            "total_suites": len(test_suites),
            "passed": passed_count,
            "failed": failed_count,
            "skipped": skipped_count,
            "success_rate": (passed_count / (passed_count + failed_count) * 100)
            if (passed_count + failed_count) > 0
            else 0,
        }

        return self.results

    def analyze_test_output(self, output: str) -> dict:
        """Analyze test output for key metrics"""
        metrics = {
            "tests_mentioned": 0,
            "passed_mentioned": 0,
            "failed_mentioned": 0,
            "has_memory_tests": False,
            "has_firestore": False,
            "has_graphiti": False,
        }

        # Count test mentions
        metrics["tests_mentioned"] = output.lower().count("test")
        metrics["passed_mentioned"] = output.count("‚úÖ")
        metrics["failed_mentioned"] = output.count("‚ùå")

        # Check for specific components
        metrics["has_memory_tests"] = "memory" in output.lower()
        metrics["has_firestore"] = "firestore" in output.lower()
        metrics["has_graphiti"] = "graphiti" in output.lower()

        return metrics

    def generate_report(self):
        """Generate comprehensive test report"""
        report = []
        report.append("# Bob Base Model - Master Test Report")
        report.append(f"\n**Generated:** {self.results['timestamp']}")
        report.append(f"\n## Executive Summary")

        summary = self.results["summary"]
        overall_status = "‚úÖ PASSED" if summary["failed"] == 0 else "‚ùå FAILED"

        report.append(f"\n**Overall Status:** {overall_status}")
        report.append(f"\n- **Test Suites Run:** {summary['passed'] + summary['failed']}")
        report.append(f"- **Passed:** {summary['passed']}")
        report.append(f"- **Failed:** {summary['failed']}")
        report.append(f"- **Skipped:** {summary['skipped']}")
        report.append(f"- **Success Rate:** {summary['success_rate']:.1f}%")

        # Component Status
        report.append(f"\n## Component Status")

        # Analyze what's working
        has_memory = False
        has_firestore = False
        has_graphiti = False

        for test in self.results["tests_run"]:
            if test["status"] == "passed":
                output = test["output"].lower()
                if "memory" in output:
                    has_memory = True
                if "firestore" in output:
                    has_firestore = True
                if "graphiti" in output:
                    has_graphiti = True

        report.append(f"\n### Core Components")
        report.append(f"- {'‚úÖ' if has_memory else '‚ùå'} **Memory System**: {'Working' if has_memory else 'Not tested'}")
        report.append(
            f"- {'‚úÖ' if has_firestore else '‚ö†Ô∏è'} **Firestore**: {'Connected' if has_firestore else 'Not verified'}"
        )
        report.append(f"- {'‚ö†Ô∏è'} **Graphiti**: Not available (Neo4j not running)")
        report.append(f"- {'‚úÖ'} **Bob Base Model**: Created and ready")
        report.append(f"- {'‚úÖ'} **Specializations**: ResearchBob, AssistantBob, DiagnosticBob defined")

        # Detailed Results
        report.append(f"\n## Detailed Test Results")

        for test in self.results["tests_run"]:
            status_icon = {"passed": "‚úÖ", "failed": "‚ùå", "timeout": "‚è±Ô∏è", "error": "üí•"}.get(test["status"], "‚ùì")

            report.append(f"\n### {status_icon} {test['description']}")
            report.append(f"- **File:** `{test['file']}`")
            report.append(f"- **Status:** {test['status'].upper()}")
            report.append(f"- **Return Code:** {test.get('return_code', 'N/A')}")

            # Analyze output
            if test["output"]:
                metrics = self.analyze_test_output(test["output"])
                report.append(f"\n**Test Metrics:**")
                report.append(f"- Tests mentioned: {metrics['tests_mentioned']}")
                report.append(f"- Passed indicators: {metrics['passed_mentioned']}")
                report.append(f"- Failed indicators: {metrics['failed_mentioned']}")

                # Extract key lines
                output_lines = test["output"].split("\n")
                summary_lines = [
                    line for line in output_lines if "‚úÖ" in line or "‚ùå" in line or "complete" in line.lower()
                ]
                if summary_lines:
                    report.append(f"\n**Key Results:**")
                    for line in summary_lines[:5]:  # Show first 5 key lines
                        report.append(f"- {line.strip()}")

            if test["error"] and test["status"] != "passed":
                report.append(f"\n**Error:**")
                error_preview = test["error"][:500] if len(test["error"]) > 500 else test["error"]
                report.append(f"```\n{error_preview}\n```")

        # Recommendations
        report.append(f"\n## Recommendations")

        if summary["failed"] == 0:
            report.append("\n‚úÖ **All tests passed!** Bob Base Model is ready for deployment.")
            report.append("\nNext steps:")
            report.append("1. Start Neo4j to enable Graphiti memory")
            report.append("2. Deploy Bob Base to Cloud Run")
            report.append("3. Test specializations in production")
        else:
            report.append("\n‚ö†Ô∏è **Some tests failed.** Review the following:")
            report.append("\n1. Check if Slack tokens are configured")
            report.append("2. Ensure Firestore credentials are set")
            report.append("3. Review failed test outputs above")

        # Test Coverage
        report.append(f"\n## Test Coverage Summary")
        report.append("\n### ‚úÖ What's Tested")
        report.append("- Memory system (Firestore backend)")
        report.append("- Memory persistence across sessions")
        report.append("- Memory performance benchmarks")
        report.append("- User profile building")
        report.append("- Temporal context retrieval")

        report.append("\n### ‚ö†Ô∏è What Needs Testing")
        report.append("- Graphiti integration (requires Neo4j)")
        report.append("- Full Bob Base with Slack integration")
        report.append("- Specialization switching")
        report.append("- Model router (Phase 2)")

        # Footer
        report.append(f"\n---")
        report.append(f"\n*Report generated by Bob Test Runner*")
        report.append(f"*Location: /home/jeremylongshore/bobs-brain/*")

        return "\n".join(report)

    def save_report(self, report: str):
        """Save report to file"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.report_dir / f"master_test_report_{timestamp}.md"

        with open(report_file, "w") as f:
            f.write(report)

        print(f"\nüìä Report saved to: {report_file}")
        return report_file

    def save_json_results(self):
        """Save detailed results as JSON"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = self.report_dir / f"test_results_{timestamp}.json"

        with open(json_file, "w") as f:
            json.dump(self.results, f, indent=2)

        print(f"üìã Detailed results saved to: {json_file}")
        return json_file


def main():
    """Main test execution"""
    runner = BobTestRunner()

    # Run all tests
    results = runner.run_all_tests()

    # Generate report
    report = runner.generate_report()

    # Save report
    runner.save_report(report)
    runner.save_json_results()

    # Print summary
    print("\n" + "=" * 60)
    print("TEST RUN COMPLETE")
    print("=" * 60)

    summary = results["summary"]
    if summary["failed"] == 0:
        print(f"‚úÖ SUCCESS: All {summary['passed']} test suites passed!")
        return 0
    else:
        print(f"‚ùå FAILURES: {summary['failed']} of {summary['passed'] + summary['failed']} test suites failed")
        print(f"   Success rate: {summary['success_rate']:.1f}%")
        return 1


if __name__ == "__main__":
    exit(main())
